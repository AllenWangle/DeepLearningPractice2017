{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用搜狗新闻语料训练三种词向量 Word2Vec,GloVe及FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "class GloVe(object):\n",
    "    def __init__(self,filename):\n",
    "        self.filename=filename\n",
    "        self.vocab_size=100\n",
    "        self.dst_filename=filename+'.glove.vec'\n",
    "        \n",
    "    def train(self):\n",
    "        import gensim,os\n",
    "        if os.path.exists(self.filename)==False:\n",
    "            print('文件不存在',self.filename)\n",
    "        else:\n",
    "            !bash tools/GloVe.sh {self.filename} {self.vocab_size}\n",
    "            \n",
    "        self._format2gensim(self.dst_filename)\n",
    "        model=gensim.models.Word2Vec.load_word2vec_format(self.dst_filename)\n",
    "        print('运行完毕，向量文件保存到:',self.dst_filename)\n",
    "        return model\n",
    "    \n",
    "    def _format2gensim(self,dst_fname):\n",
    "        src_fname='tools/GloVe/vectors.txt'\n",
    "        with open(src_fname,encoding='utf8') as src:\n",
    "            vector_size=len(src.readline().strip().split())-1\n",
    "            vocab_size=1+len(src.readlines())\n",
    "        with open(dst_fname,'w',encoding='utf8') as dst:\n",
    "            dst.write('%d %d\\n'%(vocab_size,vector_size))\n",
    "            with open(src_fname,encoding='utf8') as src:\n",
    "                for line in src:\n",
    "                    dst.write(line)\n",
    "        print(dst_fname,'已转换为gensim格式')\n",
    "        \n",
    "class FastText(object):\n",
    "    def __init__(self,filename):\n",
    "        self.filename=filename\n",
    "        self.vocab_size=100\n",
    "        self.dst_filename=filename+'.fasttext.vec'\n",
    "        \n",
    "    def train(self):\n",
    "        import gensim,os\n",
    "        if os.path.exists(self.filename)==False:\n",
    "            print('文件不存在',self.filename)\n",
    "        else:\n",
    "            !bash tools/fastText.sh {self.filename} {self.vocab_size} {self.dst_filename[:-4]}\n",
    "        \n",
    "        model=gensim.models.Word2Vec.load_word2vec_format(self.dst_filename)\n",
    "        print('运行完毕，向量文件保存到:',self.dst_filename)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import SogouCACorpus\n",
    "filename='data/news_tensite_xml.smarty.dat'\n",
    "sentences=SogouCACorpus(filename)\n",
    "w2v_model=gensim.models.Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p build\n",
      "gcc src/glove.c -o build/glove -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n",
      "gcc src/shuffle.c -o build/shuffle -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n",
      "gcc src/cooccur.c -o build/cooccur -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n",
      "gcc src/vocab_count.c -o build/vocab_count -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n",
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[0GProcessed 53210 tokens.\n",
      "Counted 10615 unique words.\n",
      "Truncating vocabulary at min count 5.\n",
      "Using vocabulary of size 1652.\n",
      "\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"vocab.txt\"...loaded 1652 words.\n",
      "Building lookup table...table contains 2729105 elements.\n",
      "Processing token: 0\u001b[0GProcessed 53210 tokens.\n",
      "Writing cooccurrences to disk......2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[0GMerging cooccurrence files: processed 303852 lines.\n",
      "\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 303852 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G303852 lines.\u001b[0GMerging temp files: processed 303852 lines.\n",
      "\n",
      "TRAINING MODEL\n",
      "Read 303852 lines.\n",
      "Initializing parameters...done.\n",
      "vector size: 100\n",
      "vocab size: 1652\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "03/02/17 - 05:38.26PM, iter: 001, cost: 0.074250\n",
      "03/02/17 - 05:38.26PM, iter: 002, cost: 0.061972\n",
      "03/02/17 - 05:38.26PM, iter: 003, cost: 0.052615\n",
      "03/02/17 - 05:38.26PM, iter: 004, cost: 0.048790\n",
      "03/02/17 - 05:38.26PM, iter: 005, cost: 0.046729\n",
      "03/02/17 - 05:38.26PM, iter: 006, cost: 0.045219\n",
      "03/02/17 - 05:38.27PM, iter: 007, cost: 0.043865\n",
      "03/02/17 - 05:38.27PM, iter: 008, cost: 0.042447\n",
      "03/02/17 - 05:38.27PM, iter: 009, cost: 0.040977\n",
      "03/02/17 - 05:38.27PM, iter: 010, cost: 0.039589\n",
      "03/02/17 - 05:38.27PM, iter: 011, cost: 0.038210\n",
      "03/02/17 - 05:38.27PM, iter: 012, cost: 0.036857\n",
      "03/02/17 - 05:38.27PM, iter: 013, cost: 0.035541\n",
      "03/02/17 - 05:38.27PM, iter: 014, cost: 0.034265\n",
      "03/02/17 - 05:38.27PM, iter: 015, cost: 0.033040\n",
      "data/news_tensite_xml.smarty.dat.jiebaresult.glove.vec 已转换为gensim格式\n",
      "运行完毕，向量文件保存到: data/news_tensite_xml.smarty.dat.jiebaresult.glove.vec\n"
     ]
    }
   ],
   "source": [
    "filename='data/news_tensite_xml.smarty.dat.jiebaresult'\n",
    "glove=GloVe(filename)\n",
    "glove_model=glove.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: 对“opt”无需做任何事。\n",
      "Read 0M words\n",
      "Number of words:  1653\n",
      "Number of labels: 0\n",
      "Progress: 100.0%  words/sec/thread: 57953  lr: 0.000000  loss: 2.669541  eta: 0h0m \n",
      "运行完毕，向量文件保存到: data/news_tensite_xml.smarty.dat.jiebaresult.fasttext.vec\n"
     ]
    }
   ],
   "source": [
    "filename='data/news_tensite_xml.smarty.dat.jiebaresult'\n",
    "fastText=FastText(filename)\n",
    "fast_model=fastText.train()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
