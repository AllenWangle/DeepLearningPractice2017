{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 面向评价对象的情感分析 TD-LSTM实现代码\n",
    "论文：Tang, Duyu, et al. \"Effective LSTMs for Target-Dependent Sentiment Classification.\" arXiv preprint arXiv:1512.01100 (2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from yuml.datasets.gridsum2016 import load_data,over_sample,patchMatrix\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import merge,Dense\n",
    "from keras import activations, initializations, regularizers, constraints\n",
    "\n",
    "class MaskAverageLayer(Layer):\n",
    "    '''得到评价对象中所有词向量的平均值\n",
    "    '''\n",
    "    def __init__(self, keepdims=True,**kwargs):\n",
    "        self.support_mask=True\n",
    "        self.keepdims=keepdims\n",
    "        super(MaskAverageLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        aspect_x=x\n",
    "        aspect_vector=(aspect_x*mask.dimshuffle(0,1,'x')).mean(axis=1,keepdims=self.keepdims)\n",
    "        return aspect_vector\n",
    "        \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.keepdims:\n",
    "            return (input_shape[0],1,input_shape[2])\n",
    "        else:\n",
    "            return (input_shape[0],input_shape[2])\n",
    "    \n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return None\n",
    "    \n",
    "class ConnectAspectLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.support_mask=True\n",
    "        super(ConnectAspectLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        left_x=x[0]\n",
    "        aspect_vector=x[1]  #32x1x100\n",
    "        aspect_vector=aspect_vector.repeat(K.shape(left_x)[1],axis=1) #32x24x100\n",
    "        aspect_vector=aspect_vector*mask[0].dimshuffle(0,1,'x')\n",
    "        return merge(inputs=[left_x,aspect_vector],mode='concat')\n",
    "        \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        \n",
    "        return (input_shape[0][0],input_shape[0][1],input_shape[0][2]+input_shape[1][2])\n",
    "    \n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if mask:\n",
    "            return mask[0]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TD_LSTM(object):\n",
    "    '''\n",
    "    基于双向LSTM的多视角分析程序\n",
    "    @2016.11.29\n",
    "    '''\n",
    "    def __init__(self,w2v,output_dim=3):\n",
    "        from sklearn.preprocessing import LabelBinarizer\n",
    "        self.w2v=w2v\n",
    "        self.word_dim=w2v.shape[1]\n",
    "        self.output_dim=output_dim\n",
    "        self.lb=LabelBinarizer()\n",
    "        self.lb.fit([0,1,2])\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        from keras.layers import Embedding,Input,merge,Merge\n",
    "        import keras.backend as K\n",
    "        import theano\n",
    "        import theano.tensor as T\n",
    "        from keras import backend as K\n",
    "        from keras.layers import Dense,Dropout,Lambda,LSTM\n",
    "        from keras.layers import Dense\n",
    "        from keras.models import Model\n",
    "        import keras\n",
    "\n",
    "        left_input=Input(shape=(None,),dtype='int32',name='left_input')\n",
    "        right_input=Input(shape=(None,),dtype='int32',name='right_input')\n",
    "        aspect_input=Input(shape=(None,),dtype='int32',name='aspect_input')   #32x100\n",
    "\n",
    "        #词向量Embedding\n",
    "        layer=Embedding(input_dim=self.w2v.shape[0],output_dim=self.word_dim,weights=[self.w2v],mask_zero=True,name='WordEmbedding')\n",
    "        left_x=layer(left_input) #32x24x100\n",
    "        right_x=layer(right_input) #32x104x100\n",
    "        aspect_x=layer(aspect_input) #32x4x100\n",
    "\n",
    "        aspect_vector=MaskAverageLayer()(aspect_x) #32x1x100\n",
    "\n",
    "        left_mx=ConnectAspectLayer()([left_x,aspect_vector]) #32x24x200\n",
    "\n",
    "        right_mx=ConnectAspectLayer()([right_x,aspect_vector]) #32x24x200\n",
    "\n",
    "        left_vector=LSTM(output_dim=200,dropout_W=0.3,dropout_U=0.3,activation='tanh')(left_mx) #32x100\n",
    "        right_vector=LSTM(output_dim=200,go_backwards=True,dropout_W=0.3,dropout_U=0.3,activation='tanh')(right_mx) #32x100\n",
    "\n",
    "        lstm_output=merge(inputs=[left_vector,right_vector],mode='concat')\n",
    "        x=Dropout(0.5)(lstm_output)\n",
    "        x=Dense(50,activation='tanh')(x)\n",
    "        x=Dropout(0.5)(x)\n",
    "        output=Dense(self.output_dim,activation='softmax')(x)\n",
    "\n",
    "        model=Model(input=[left_input,right_input,aspect_input],output=output)\n",
    "        model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "        self.get_lstm_output=K.function(inputs=[left_input,right_input,aspect_input,K.learning_phase()],outputs=[lstm_output,output])\n",
    "        self.model=model\n",
    "        \n",
    "\n",
    "    def train(self,patched_docs,patched_ys,epoch=500,class_weight=None):\n",
    "        model=self.model\n",
    "        n_batch=len(patched_docs)\n",
    "        indexes=np.random.randint(low=0,high=n_batch,size=(epoch,))\n",
    "        loss,acc,total=0,0,0\n",
    "        for n,i in enumerate(indexes):\n",
    "            val=model.train_on_batch(patched_docs[i],patched_ys[i],class_weight=class_weight)\n",
    "            num=len(patched_docs[i])\n",
    "            loss,acc,total=loss+val[0]*num,acc+val[1]*num,total+num\n",
    "            if (n+1)%100==0:\n",
    "                print('\\r%d/%d'%(n+1,epoch),val[0],val[1],end='')\n",
    "        loss,acc=loss/total,acc/total\n",
    "        return loss,acc\n",
    "\n",
    "    def test(self,patched_docs,patched_ys):\n",
    "        model=self.model\n",
    "        loss,acc,total=0,0,0\n",
    "        for x_test,y_test in zip(patched_docs,patched_ys):\n",
    "            val=model.test_on_batch(x_test,y_test)\n",
    "            num=len(x_test)\n",
    "            loss,acc,total=loss+val[0]*num,acc+val[1]*num,total+num\n",
    "        loss,acc=loss/total,acc/total\n",
    "        return loss,acc\n",
    "\n",
    "    def predict(self,patched_docs,patched_ids):\n",
    "        \n",
    "        results=[]\n",
    "        for x_test,ids in zip(patched_docs,patched_ids):\n",
    "            val=self.model.predict_on_batch(x_test)\n",
    "            results.extend(zip(val,ids))\n",
    "        return results\n",
    "\n",
    "    def fit(self,train_data,valid_data=None,class_weight=None,n_earlystop=30,filename='best.model',\n",
    "            cnt_in_epoch=100,n_epoch=500,best_type='best_loss'):\n",
    "        model=self.model\n",
    "        best_loss=1000\n",
    "        best_epoch=0\n",
    "        best_acc=0\n",
    "        early_stop=0\n",
    "        n_stop=n_earlystop\n",
    "        import datetime\n",
    "        for i in range(n_epoch):\n",
    "            early_stop+=1\n",
    "            val=self.train(train_data[0],train_data[1],cnt_in_epoch,class_weight)\n",
    "            print('\\r',i+1,'train',val)\n",
    "            if valid_data:\n",
    "                print('testing...',end='')\n",
    "                val=self.test(valid_data[0],valid_data[1])\n",
    "                if (val[0]<best_loss and best_type=='best_loss') or (val[1]>best_acc and best_type=='best_acc'):\n",
    "                    print(best_type)\n",
    "                    best_loss=val[0]\n",
    "                    best_epoch=i\n",
    "                    best_acc=val[1]\n",
    "                    model.save_weights(filename)\n",
    "                    early_stop=0\n",
    "                t=datetime.datetime.now().strftime('%H:%M:%S')\n",
    "                print('\\r',i+1,'test',t,'loss:%f, acc:%f'%val)\n",
    "                print('-----')\n",
    "                if early_stop>n_stop:\n",
    "                    print('early stop')\n",
    "                    break\n",
    "        if valid_data:\n",
    "            print('best:',best_epoch,best_loss,best_acc)\n",
    "            self.model.load_weights(filename)\n",
    "        else:\n",
    "            best_epoch=n_epoch\n",
    "            best_loss=val[0]\n",
    "            best_acc=val[1]\n",
    "            self.model.save_weights(filename)\n",
    "        return best_epoch,best_loss,best_acc\n",
    "    \n",
    "    def get_patched_data(self,valid_data,is_train=True):\n",
    "        patched_data=[]\n",
    "        batch_size=32\n",
    "        opinions=['neg','neu','pos']\n",
    "        n_batch=int((len(valid_data)-1)/batch_size+1)\n",
    "        for i in range(n_batch):\n",
    "            items=valid_data[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            leftIds=patchMatrix(items['LeftIds']+items['ViewIds'])\n",
    "            rightIds=patchMatrix(items['ViewIds']+items['RightIds'])\n",
    "            views=patchMatrix(items.ViewIds.tolist())\n",
    "            leftPOS=patchMatrix(items['LeftPOS']+items['ViewPOS'])\n",
    "            rightPOS=patchMatrix(items['ViewPOS']+items['RightPOS'])\n",
    "            \n",
    "            if is_train:\n",
    "                ys=items.Opinion.apply(lambda x:opinions.index(x)).tolist()\n",
    "            else:\n",
    "                ys=items.Opinion.apply(lambda x:-1).tolist()\n",
    "            patched_data.append((leftIds,rightIds,views,leftPOS,rightPOS,items.index.tolist(),items.SentenceId.tolist(),ys))\n",
    "        return patched_data\n",
    "    \n",
    "\n",
    "    def get_xs(self,train_df,is_train=True):\n",
    "        train_data=self.get_patched_data(train_df,is_train)\n",
    "        train_xs=[list(item)[:3] for item in train_data]\n",
    "        train_ys=[self.lb.transform(item[-1]) for item in train_data]\n",
    "        return train_xs,train_ys\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TD_LSTM神经网络模型\n",
    "输入process_data处理好的pkl文件，包括(data,vocs,id2words,w2v)\n",
    "输出情感分析结果\n",
    "by: liyumeng\n",
    "@ 2016/11/28\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    input_filename='data/car_review_data.pkl'\n",
    "    model_filename='best.tc_lstm.model'\n",
    "    output_filename='answer.csv'\n",
    "    retrain='1'\n",
    "    \n",
    "    print('input',input_filename)\n",
    "    if retrain=='1':\n",
    "        print('retrain and save model to',model_filename)\n",
    "    else:\n",
    "        print('load model from ',model_filename)\n",
    "    print('output result to',output_filename)\n",
    "    \n",
    "    #----------------------------------------------------------\n",
    "    '''读取训练数据'''\n",
    "    train_df,valid_df,w2v,id2words,id2pos=load_data(input_filename,pos_rate=0.2,neg_rate=0.2)\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    '''开始训练'''\n",
    "\n",
    "    model=TD_LSTM(w2v)\n",
    "    train_data=model.get_xs(train_df)\n",
    "    valid_data=model.get_xs(valid_df)\n",
    "    test_data=valid_data\n",
    "    #test_data=model.get_xs(test_df,False)\n",
    "    if retrain =='1':\n",
    "        model.fit(train_data=train_data,valid_data=valid_data,filename=model_filename,best_type='best_acc')\n",
    "    else:\n",
    "        model.model.load_weights(model_filename)\n",
    "\n",
    "    #-------------------------------------------------------------------\n",
    "    '''预测并输出\n",
    "    '''\n",
    "    \n",
    "    res=model.predict(test_data[0],test_data[1])\n",
    "\n",
    "    opinions=['neg','neu','pos']\n",
    "    yp=[opinions[np.argmax(r[0])] for r in res]\n",
    "    \n",
    "    print('输出的各类别比例：')\n",
    "    test_num=Counter(yp)\n",
    "    for key in test_num:\n",
    "        print(key,test_num[key]/len(test_df),test_num[key])\n",
    "    \n",
    "    test_df.loc[:,'Opinion']=yp\n",
    "    test_df.loc[:,['SentenceId','RawView','Opinion']].to_csv(output_filename,index=False,sep=',',encoding='utf8',header=True)\n",
    "    print('运行完毕！已输出到',output_filename)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
