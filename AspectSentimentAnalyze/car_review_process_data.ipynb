{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据总数 20581\n",
      "neg 1527 0.07419464554686361\n",
      "neu 14288 0.6942325445799524\n",
      "pos 4766 0.23157280987318402\n"
     ]
    }
   ],
   "source": [
    "'''基于多视角的情感分析程序\n",
    "by:liyumeng @2017.03.08\n",
    "'''\n",
    "import os\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import jieba.posseg as pseg\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def read_train(f_train,f_label):\n",
    "    labels=pd.read_csv(f_label,sep='\\t')\n",
    "    train_data=pd.read_csv(f_train,sep='\\t')\n",
    "    data=pd.merge(labels,train_data,how='left',on='SentenceId')\n",
    "    return data[data.Content.notnull()].reset_index(drop=True)\n",
    "\n",
    "train_d1=read_train('data/CarReview1.csv','data/CarAspect1.csv')\n",
    "train_d2=read_train('data/CarReview2.csv','data/CarAspect2.csv')\n",
    "# 去除视角不在原句中的\n",
    "has_view=[]\n",
    "for i in range(len(train_d1)):\n",
    "    if train_d1.loc[i,'View'] not in train_d1.loc[i,'Content']:\n",
    "        has_view.append(False)\n",
    "    else:\n",
    "        has_view.append(True)\n",
    "\n",
    "train_d1=train_d1[has_view].reset_index(drop=True)\n",
    "train_data=pd.concat([train_d1,train_d2],ignore_index=True)\n",
    "\n",
    "print('训练集数据总数',len(train_data))\n",
    "vals=Counter(train_data.Opinion.tolist())\n",
    "for key in vals:\n",
    "    print(key,vals[key],vals[key]/len(train_data))\n",
    "data=train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Left']=pd.Series(name='Left',index=data.index,data=['' for _ in range(len(data))])\n",
    "data['Right']=pd.Series(name='Right',index=data.index,data=['' for _ in range(len(data))])\n",
    "\n",
    "for i in range(len(data)):\n",
    "    content=data.loc[i,'Content']\n",
    "    index=content.find(data.loc[i,'View'])\n",
    "    if index<0:\n",
    "        print(data.loc[i,'SentenceId'])\n",
    "    data.set_value(i,'Left',content[:index])\n",
    "    data.set_value(i,'Right',content[index+len(data.loc[i,'View']):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\yuml\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.070 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词处理...\n"
     ]
    }
   ],
   "source": [
    "views=pd.read_csv('data/CarAspectDict.csv',header=None)[0].tolist()\n",
    "'''自定义词典\n",
    "'''\n",
    "for word in views:\n",
    "    for w in word.split(' '):\n",
    "        if len(w)>0:\n",
    "            jieba.add_word(w,tag='nz')\n",
    "'''分词\n",
    "'''\n",
    "print('分词处理...')\n",
    "\n",
    "data['LeftWords']=data['Left'].apply(lambda x:pseg.lcut(x))\n",
    "data['RightWords']=data['Right'].apply(lambda x:pseg.lcut(x))\n",
    "data['ViewWords']=data['View'].apply(lambda x:pseg.lcut(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuml\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:843: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "'''载入词向量\n",
    "'''\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "w2v_model=Word2Vec.load('data/car100_v2.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''生成词表\n",
    "'''\n",
    "vocs={'\\s':0}\n",
    "id2words=['\\s']\n",
    "word_dim=w2v_model.vector_size\n",
    "w2v=[np.zeros((word_dim,))]\n",
    "\n",
    "def get_sentence_vector(w2v_model,words,i,default_dim,random=False):\n",
    "    '''传入word2vec模型以及词序列\n",
    "    返回窗口内的平均向量\n",
    "    返回句子中平均词向量，用来填充缺失的词向量\n",
    "    '''\n",
    "    if random:\n",
    "        return np.random.uniform(low=-0.01,high=0.01,size=(default_dim,))\n",
    "    else:\n",
    "        window_size=5\n",
    "        start=max(0,i-window_size)\n",
    "        end=min(i+window_size,len(words))\n",
    "        vectors=[w2v_model[w] for w in words[start:end] if w in w2v_model]\n",
    "        if len(vectors)==0:\n",
    "            return np.random.uniform(low=-0.01,high=0.01,size=(default_dim,))\n",
    "        return np.average(vectors,axis=0)\n",
    "\n",
    "def get_wids(words):\n",
    "    '''获得评价内容中词语的Id\n",
    "    '''\n",
    "    wids=[]\n",
    "    for i,word in enumerate(words):\n",
    "        w=word.word\n",
    "        if w not in vocs:\n",
    "            vocs[w]=len(vocs)\n",
    "            id2words.append(w)\n",
    "            if w not in w2v_model:\n",
    "                sen_vector=get_sentence_vector(w2v_model,words,i,word_dim)\n",
    "                w2v.append(sen_vector)\n",
    "            else:\n",
    "                w2v.append(w2v_model[w])\n",
    "            \n",
    "        wids.append(vocs[w])\n",
    "    return wids\n",
    "\n",
    "pos_dict={'\\s':0}\n",
    "id2pos=['\\s']\n",
    "def get_pos(words):\n",
    "    '''获取评价内容中词语词性的Id\n",
    "    '''\n",
    "    pos_list=[]\n",
    "    for i,word in enumerate(words):\n",
    "        pos=word.flag\n",
    "        if pos not in pos_dict:\n",
    "            pos_dict[pos]=len(pos_dict)\n",
    "            id2pos.append(pos)\n",
    "        pos_list.append(pos_dict[pos])\n",
    "    return pos_list\n",
    "    \n",
    "data.loc[:,'LeftIds']=data.LeftWords.apply(lambda x:get_wids(x))\n",
    "data.loc[:,'RightIds']=data.RightWords.apply(lambda x:get_wids(x))\n",
    "data.loc[:,'ViewIds']=data.ViewWords.apply(lambda x:get_wids(x))\n",
    "\n",
    "data.loc[:,'LeftPOS']=data.LeftWords.apply(lambda x:get_pos(x))\n",
    "data.loc[:,'RightPOS']=data.RightWords.apply(lambda x:get_pos(x))\n",
    "data.loc[:,'ViewPOS']=data.ViewWords.apply(lambda x:get_pos(x))\n",
    "\n",
    "w2v=np.array(w2v,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''用于Memory网络的位置权重\n",
    "'''\n",
    "positions=[]\n",
    "for row in data.itertuples():\n",
    "    position=[]\n",
    "    n=len(row.LeftIds+row.ViewIds+row.RightIds)\n",
    "    right_start=len(row.LeftIds)+len(row.ViewIds)\n",
    "    for i in range(len(row.LeftIds)):\n",
    "        position.append(1-i/n)\n",
    "    for i in range(len(row.RightIds)):\n",
    "        position.append(1-(i+right_start)/n)\n",
    "    positions.append(position)\n",
    "\n",
    "data['Positions']=pd.Series(data=positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''保存处理好的训练数据到文件中\n",
    "格式：\n",
    "  data  pandas的DataFrame\n",
    "  w2v   numpy array\n",
    "  vocs  dict\n",
    "  id2words   list\n",
    "  id2pos     list\n",
    "'''\n",
    "import pickle\n",
    "pickle.dump([data,w2v,vocs,id2words,id2pos],open('data/car_review_data.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
