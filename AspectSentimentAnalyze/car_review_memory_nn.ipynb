{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 面向评价对象的情感分析\n",
    "论文：Tang, Duyu, Bing Qin, and Ting Liu. \"Aspect level sentiment classification with deep memory network.\" arXiv preprint arXiv:1605.08900 (2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from yuml.datasets.gridsum2016 import load_data,over_sample,patchMatrix\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import merge,Dense\n",
    "from keras import activations, initializations, regularizers, constraints\n",
    "\n",
    "def softmask(x, mask,axis=-1):\n",
    "    '''softmax with mask, used in attention mechanism others\n",
    "    '''\n",
    "    y = T.exp(x)\n",
    "    if mask:\n",
    "        y = y * mask\n",
    "    sumx = T.sum(y, axis=axis, keepdims=True) + 1e-6\n",
    "    x = y / sumx\n",
    "    return x\n",
    "\n",
    "\n",
    "class PositionWeightLayer(Layer):\n",
    "    '''根据词在句子中的位置，给词向量加权\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        self.support_mask=True\n",
    "        super(PositionWeightLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        word_emb=x[0]\n",
    "        weights=x[1]\n",
    "        word_emb=word_emb*weights.dimshuffle(0,1,'x')\n",
    "        return word_emb\n",
    "        \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape[0]\n",
    "    \n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if mask:\n",
    "            return mask[0]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "class MaskAverageLayer(Layer):\n",
    "    '''得到评价对象中所有词向量的平均值\n",
    "    '''\n",
    "    def __init__(self, keepdims=True,**kwargs):\n",
    "        self.support_mask=True\n",
    "        self.keepdims=keepdims\n",
    "        super(MaskAverageLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        aspect_x=x\n",
    "        aspect_vector=(aspect_x*mask.dimshuffle(0,1,'x')).mean(axis=1,keepdims=self.keepdims)\n",
    "        return aspect_vector\n",
    "        \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.keepdims:\n",
    "            return (input_shape[0],1,input_shape[2])\n",
    "        else:\n",
    "            return (input_shape[0],input_shape[2])\n",
    "    \n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return None\n",
    "    \n",
    "class MemoryLayer(Layer):\n",
    "    '''\n",
    "    Input: memory 32x6x310\n",
    "    x is aspect vector with shape(32x310)\n",
    "    Output:\n",
    "    shape(32x310)\n",
    "    '''\n",
    "    def __init__(self,mask=None,W_regularizer=None,b_regularizer=None,**kwargs):\n",
    "        self.supports_masking = False\n",
    "        self.mask=mask\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        super(MemoryLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        aspect_shape=input_shape[0]\n",
    "        n_in=aspect_shape[1]*2\n",
    "        n_out=1\n",
    "        lim = np.sqrt(6. / (n_in + n_out))\n",
    "        self.W = K.random_uniform_variable((n_in, n_out), -lim, lim, name='{}_W'.format(self.name))\n",
    "        self.b = K.zeros((n_out,), name='{}_b'.format(self.name))\n",
    "        self.trainable_weights = [self.W, self.b]\n",
    "        \n",
    "        self.regularizers = []\n",
    "        if self.W_regularizer:\n",
    "            self.W_regularizer.set_param(self.W)\n",
    "            self.regularizers.append(self.W_regularizer)\n",
    "\n",
    "        if self.b_regularizer:\n",
    "            self.b_regularizer.set_param(self.b)\n",
    "            self.regularizers.append(self.b_regularizer)\n",
    "        pass\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        aspect=x[0]\n",
    "        memory=x[1]\n",
    "        vaspect=K.repeat(aspect,K.shape(memory)[1])\n",
    "        x=merge(inputs=[memory,vaspect],mode='concat')\n",
    "        gi = K.tanh(K.dot(x, self.W) + self.b) #32x6x1\n",
    "        gi=K.sum(gi,axis=-1)  #32x6\n",
    "        ai = softmask(gi, self.mask,axis=-1)  # 32x6\n",
    "        output=K.sum(memory*ai.dimshuffle(0,1,'x'),axis=-2)\n",
    "        return output\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape[0]\n",
    "    \n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MemoryNNModel(object):\n",
    "    '''\n",
    "    基于Memory的视角级别情感分析\n",
    "    @2016.11.28\n",
    "    '''\n",
    "    def __init__(self,w2v,n_hop=4):\n",
    "        self.n_hop=n_hop\n",
    "        self.w2v=w2v\n",
    "        \n",
    "        self.word_dim=100\n",
    "       \n",
    "        self.n_class=3\n",
    "        \n",
    "        self.lb=LabelBinarizer()\n",
    "        self.lb.fit([0,1,2])\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        import keras\n",
    "        import keras.backend as K\n",
    "        from keras.layers import Embedding,Input,merge,Merge,Dense,Dropout\n",
    "        from keras.engine.topology import Layer\n",
    "        from keras import activations, initializations, regularizers, constraints\n",
    "        from keras.models import Model\n",
    "        import theano\n",
    "\n",
    "\n",
    "        word_input=Input(shape=(None,),dtype='int32',name='word_input')\n",
    "        position_input=Input(shape=(None,),dtype='float32',name='position_input')\n",
    "        aspect_input=Input(shape=(None,),dtype='int32',name='aspect_input')   #32x100\n",
    "\n",
    "\n",
    "        #词向量Embedding\n",
    "        layer=Embedding(input_dim=self.w2v.shape[0],output_dim=self.word_dim,weights=[self.w2v],mask_zero=True,name='WordEmbedding')\n",
    "        word_x=layer(word_input)\n",
    "        aspect_x=layer(aspect_input)\n",
    "        mask=layer.get_output_mask_at(0) #32x6\n",
    "\n",
    "        word_x=PositionWeightLayer()([word_x,position_input])\n",
    "\n",
    "        aspect_vector=MaskAverageLayer(keepdims=False)(aspect_x) #32x100\n",
    "\n",
    "        #提取aspect向量表示及memory矩阵\n",
    "        aspect_dim=self.word_dim\n",
    "        memory=word_x #32x6x110\n",
    "\n",
    "        x=aspect_vector\n",
    "        x=Dense(aspect_dim)(x)\n",
    "        x=MemoryLayer(mask)([x,memory])\n",
    "        x=Dropout(0.5)(x)\n",
    "\n",
    "        for i in range(self.n_hop):\n",
    "            x=Dense(aspect_dim)(x)\n",
    "            x=MemoryLayer(mask)([x,memory])\n",
    "            x=Dropout(0.5)(x)\n",
    "        memory_output=x\n",
    "\n",
    "        output=Dense(3,activation='softmax')(x)\n",
    "        model=Model(input=[word_input,position_input,aspect_input],output=output)\n",
    "        model.compile(optimizer='nadam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "        self.model=model\n",
    "        #self.get_memory_output=K.function(inputs=[word_input,position_input,aspect_input,POS_input,K.learning_phase()],outputs=[memory_output,output])\n",
    "\n",
    "    def train(self,patched_docs,patched_ys,epoch=500,class_weight=None):\n",
    "        model=self.model\n",
    "        n_batch=len(patched_docs)\n",
    "        indexes=np.random.randint(low=0,high=n_batch,size=(epoch,))\n",
    "        loss,acc,total=0,0,0\n",
    "        for n,i in enumerate(indexes):\n",
    "            val=model.train_on_batch(patched_docs[i],patched_ys[i],class_weight=class_weight)\n",
    "            #val=model.train_on_batch(patched_docs[i],patched_ys[i])\n",
    "            num=len(patched_docs[i])\n",
    "            loss,acc,total=loss+val[0]*num,acc+val[1]*num,total+num\n",
    "            if (n+1)%100==0:\n",
    "                print('\\r%d/%d'%(n+1,epoch),val[0],val[1],end='')\n",
    "        loss,acc=loss/total,acc/total\n",
    "        return loss,acc\n",
    "\n",
    "    def test(self,patched_docs,patched_ys):\n",
    "        model=self.model\n",
    "        loss,acc,total=0,0,0\n",
    "        for x_test,y_test in zip(patched_docs,patched_ys):\n",
    "            val=model.test_on_batch(x_test,y_test)\n",
    "            num=len(x_test)\n",
    "            loss,acc,total=loss+val[0]*num,acc+val[1]*num,total+num\n",
    "        loss,acc=loss/total,acc/total\n",
    "        return loss,acc\n",
    "\n",
    "    def predict(self,patched_docs,patched_ids):\n",
    "        \n",
    "        results=[]\n",
    "        for x_test,ids in zip(patched_docs,patched_ids):\n",
    "            val=self.model.predict_on_batch(x_test)\n",
    "            results.extend(zip(val,ids))\n",
    "        return results\n",
    "\n",
    "    def fit(self,train_data,valid_data=None,class_weight=None,n_earlystop=30,filename='best.model',n_epoch=500,best_type='best_acc'):\n",
    "        model=self.model\n",
    "        best_loss=1000\n",
    "        best_epoch=0\n",
    "        best_acc=0\n",
    "        early_stop=0\n",
    "        n_stop=n_earlystop\n",
    "        if class_weight==None:\n",
    "            class_weight=[1,1,1]\n",
    "        import datetime\n",
    "        for i in range(n_epoch):\n",
    "            early_stop+=1\n",
    "            val=self.train(train_data[0],train_data[1],500,class_weight)\n",
    "            print('\\r',i+1,'train',val)\n",
    "            if valid_data:\n",
    "                print('testing...',end='')\n",
    "                val=self.test(valid_data[0],valid_data[1])\n",
    "                if (val[0]<best_loss and best_type=='best_loss') or (val[1]>best_acc and best_type=='best_acc'):\n",
    "                    print(best_type)\n",
    "                    best_loss=val[0]\n",
    "                    best_epoch=i\n",
    "                    best_acc=val[1]\n",
    "                    model.save_weights(filename)\n",
    "                    early_stop=0\n",
    "                t=datetime.datetime.now().strftime('%H:%M:%S')\n",
    "                print('\\r',i+1,'test',t,'loss:%f, acc:%f'%val)\n",
    "                print('-----')\n",
    "                if early_stop>n_stop:\n",
    "                    print('early stop')\n",
    "                    break\n",
    "        if valid_data:\n",
    "            print('best:',best_epoch,best_loss,best_acc)\n",
    "            self.model.load_weights(filename)\n",
    "        else:\n",
    "            best_epoch=n_epoch\n",
    "            best_loss=val[0]\n",
    "            best_acc=val[1]\n",
    "            self.model.save_weights(filename)\n",
    "        return best_epoch,best_loss,best_acc\n",
    "\n",
    "\n",
    "    def get_patched_data(self,valid_data,is_train=True):\n",
    "        patched_data=[]\n",
    "        batch_size=32\n",
    "        opinions=['neg','neu','pos']\n",
    "        n_batch=int((len(valid_data)-1)/batch_size+1)\n",
    "        for i in range(n_batch):\n",
    "            items=valid_data[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            wordIds=patchMatrix(items['LeftIds']+items['RightIds'])\n",
    "            positions=patchMatrix(items['Positions'],dtype=np.float32)\n",
    "            views=patchMatrix(items.ViewIds.tolist())\n",
    "            \n",
    "            if is_train:\n",
    "                ys=items.Opinion.apply(lambda x:opinions.index(x)).tolist()\n",
    "            else:\n",
    "                ys=items.Opinion.apply(lambda x:-1).tolist()\n",
    "            patched_data.append((wordIds,positions,views,items.index.tolist(),items.SentenceId.tolist(),ys))\n",
    "        return patched_data\n",
    "    \n",
    "\n",
    "    def get_xs(self,train_df,is_train=True):\n",
    "        train_data=self.get_patched_data(train_df,is_train)\n",
    "        train_xs=[list(item)[:3] for item in train_data]\n",
    "        train_ys=[self.lb.transform(item[-1]) for item in train_data]\n",
    "        return train_xs,train_ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Memory-Attention神经网络模型\n",
    "输入process_data处理好的pkl文件，包括(train_df,valid_df,w2v,id2words,id2pos)\n",
    "输出情感分析结果\n",
    "by: liyumeng\n",
    "@ 2017/3/11\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "if __name__=='__main__':\n",
    "    input_filename='data/car_review_data.pkl'\n",
    "    model_filename='best.memory_nn.model'\n",
    "    output_filename='answer.csv'\n",
    "    retrain='1'\n",
    "    \n",
    "    print('input',input_filename)\n",
    "    if retrain=='1':\n",
    "        print('retrain and save model to',model_filename)\n",
    "    else:\n",
    "        print('load model from ',model_filename)\n",
    "    print('output result to',output_filename)\n",
    "    \n",
    "    #----------------------------------------------------------\n",
    "    '''读取训练数据'''\n",
    "    train_df,valid_df,w2v,id2words,id2pos=load_data(input_filename,pos_rate=0.2,neg_rate=0.2)\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    '''开始训练'''\n",
    "\n",
    "    model=MemoryNNModel(w2v)\n",
    "    train_data=model.get_xs(train_df)\n",
    "    valid_data=model.get_xs(valid_df)\n",
    "    test_data=valid_data\n",
    "    #test_data=model.get_xs(test_df,False)\n",
    "    if retrain =='1':\n",
    "        model.fit(train_data=train_data,valid_data=valid_data,filename=model_filename,best_type='best_acc')\n",
    "    else:\n",
    "        model.model.load_weights(model_filename)\n",
    "\n",
    "    #-------------------------------------------------------------------\n",
    "    '''预测并输出\n",
    "    '''\n",
    "    \n",
    "    res=model.predict(test_data[0],test_data[1])\n",
    "\n",
    "    opinions=['neg','neu','pos']\n",
    "    yp=[opinions[np.argmax(r[0])] for r in res]\n",
    "    \n",
    "    print('输出的各类别比例：')\n",
    "    test_num=Counter(yp)\n",
    "    for key in test_num:\n",
    "        print(key,test_num[key]/len(test_df),test_num[key])\n",
    "    \n",
    "    test_df.loc[:,'Opinion']=yp\n",
    "    test_df.loc[:,['SentenceId','RawView','Opinion']].to_csv(output_filename,index=False,sep=',',encoding='utf8',header=True)\n",
    "    print('运行完毕！已输出到',output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 测试代码\n",
    "```\n",
    "import theano\n",
    "\n",
    "f=theano.function(inputs=[word_input,position_input,aspect_input,K.learning_phase()],outputs=[output])\n",
    "\n",
    "test_word_input=train_data[0][0][0]\n",
    "test_position_input=train_data[0][0][1]\n",
    "test_aspect_input=train_data[0][0][2]\n",
    "\n",
    "results=f(test_word_input,test_position_input,test_aspect_input,0)\n",
    "print('result shape:',results[0].shape)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
