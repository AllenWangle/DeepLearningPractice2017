{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TC_LSTM(object):\n",
    "    '''\n",
    "    基于双向LSTM的多视角分析程序\n",
    "    @2016.11.29\n",
    "    '''\n",
    "    def __init__(self,w2v,output_dim=3):\n",
    "        from sklearn.preprocessing import LabelBinarizer\n",
    "        self.w2v=w2v\n",
    "        self.word_dim=w2v.shape[1]\n",
    "        self.output_dim=output_dim\n",
    "        self.lb=LabelBinarizer()\n",
    "        self.lb.fit([0,1,2])\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        from keras.layers import Embedding,Input,merge,Merge\n",
    "        import keras.backend as K\n",
    "        import theano\n",
    "        import theano.tensor as T\n",
    "        from keras import backend as K\n",
    "        from keras.layers import Dense,Dropout,Lambda,LSTM\n",
    "        from yuml.keras.layers import MaskMeanLayer,ConnectAspectLayer\n",
    "        from keras.layers import Dense\n",
    "        from keras.models import Model\n",
    "        import keras\n",
    "\n",
    "        left_input=Input(shape=(None,),dtype='int32',name='left_input')\n",
    "        right_input=Input(shape=(None,),dtype='int32',name='right_input')\n",
    "        aspect_input=Input(shape=(None,),dtype='int32',name='aspect_input')   #32x100\n",
    "\n",
    "        #词向量Embedding\n",
    "        layer=Embedding(input_dim=self.w2v.shape[0],output_dim=self.word_dim,weights=[self.w2v],mask_zero=True,name='WordEmbedding')\n",
    "        left_x=layer(left_input) #32x24x100\n",
    "        right_x=layer(right_input) #32x104x100\n",
    "        aspect_x=layer(aspect_input) #32x4x100\n",
    "\n",
    "        aspect_vector=MaskMeanLayer()(aspect_x) #32x1x100\n",
    "\n",
    "        left_mx=ConnectAspectLayer()([left_x,aspect_vector]) #32x24x200\n",
    "\n",
    "        right_mx=ConnectAspectLayer()([right_x,aspect_vector]) #32x24x200\n",
    "\n",
    "        left_vector=LSTM(output_dim=200,dropout_W=0.3,dropout_U=0.3,activation='tanh')(left_mx) #32x100\n",
    "        right_vector=LSTM(output_dim=200,go_backwards=True,dropout_W=0.3,dropout_U=0.3,activation='tanh')(right_mx) #32x100\n",
    "\n",
    "        lstm_output=merge(inputs=[left_vector,right_vector],mode='concat')\n",
    "        x=Dropout(0.5)(lstm_output)\n",
    "        x=Dense(50,activation='tanh')(x)\n",
    "        x=Dropout(0.5)(x)\n",
    "        output=Dense(self.output_dim,activation='softmax')(x)\n",
    "\n",
    "        model=Model(input=[left_input,right_input,aspect_input],output=output)\n",
    "        model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "        self.get_lstm_output=K.function(inputs=[left_input,right_input,aspect_input,K.learning_phase()],outputs=[lstm_output,output])\n",
    "        self.model=model\n",
    "        \n",
    "\n",
    "    def train(self,patched_docs,patched_ys,epoch=500,class_weight=None):\n",
    "        model=self.model\n",
    "        n_batch=len(patched_docs)\n",
    "        indexes=np.random.randint(low=0,high=n_batch,size=(epoch,))\n",
    "        loss,acc,total=0,0,0\n",
    "        for n,i in enumerate(indexes):\n",
    "            val=model.train_on_batch(patched_docs[i],patched_ys[i],class_weight=class_weight)\n",
    "            num=len(patched_docs[i])\n",
    "            loss,acc,total=loss+val[0]*num,acc+val[1]*num,total+num\n",
    "            if (n+1)%100==0:\n",
    "                print('\\r%d/%d'%(n+1,epoch),val[0],val[1],end='')\n",
    "        loss,acc=loss/total,acc/total\n",
    "        return loss,acc\n",
    "\n",
    "    def test(self,patched_docs,patched_ys):\n",
    "        model=self.model\n",
    "        loss,acc,total=0,0,0\n",
    "        for x_test,y_test in zip(patched_docs,patched_ys):\n",
    "            val=model.test_on_batch(x_test,y_test)\n",
    "            num=len(x_test)\n",
    "            loss,acc,total=loss+val[0]*num,acc+val[1]*num,total+num\n",
    "        loss,acc=loss/total,acc/total\n",
    "        return loss,acc\n",
    "\n",
    "    def predict(self,patched_docs,patched_ids):\n",
    "        \n",
    "        results=[]\n",
    "        for x_test,ids in zip(patched_docs,patched_ids):\n",
    "            val=self.model.predict_on_batch(x_test)\n",
    "            results.extend(zip(val,ids))\n",
    "        return results\n",
    "\n",
    "    def fit(self,train_data,valid_data=None,class_weight=None,n_earlystop=30,filename='best.model',\n",
    "            cnt_in_epoch=100,n_epoch=500,best_type='best_loss'):\n",
    "        model=self.model\n",
    "        best_loss=1000\n",
    "        best_epoch=0\n",
    "        best_acc=0\n",
    "        early_stop=0\n",
    "        n_stop=n_earlystop\n",
    "        import datetime\n",
    "        for i in range(n_epoch):\n",
    "            early_stop+=1\n",
    "            val=self.train(train_data[0],train_data[1],cnt_in_epoch,class_weight)\n",
    "            print('\\r',i+1,'train',val)\n",
    "            if valid_data:\n",
    "                print('testing...',end='')\n",
    "                val=self.test(valid_data[0],valid_data[1])\n",
    "                if (val[0]<best_loss and best_type=='best_loss') or (val[1]>best_acc and best_type=='best_acc'):\n",
    "                    print(best_type)\n",
    "                    best_loss=val[0]\n",
    "                    best_epoch=i\n",
    "                    best_acc=val[1]\n",
    "                    model.save_weights(filename)\n",
    "                    early_stop=0\n",
    "                t=datetime.datetime.now().strftime('%H:%M:%S')\n",
    "                print('\\r',i+1,'test',t,'loss:%f, acc:%f'%val)\n",
    "                print('-----')\n",
    "                if early_stop>n_stop:\n",
    "                    print('early stop')\n",
    "                    break\n",
    "        if valid_data:\n",
    "            print('best:',best_epoch,best_loss,best_acc)\n",
    "            self.model.load_weights(filename)\n",
    "        else:\n",
    "            best_epoch=n_epoch\n",
    "            best_loss=val[0]\n",
    "            best_acc=val[1]\n",
    "            self.model.save_weights(filename)\n",
    "        return best_epoch,best_loss,best_acc\n",
    "    \n",
    "    def get_patched_data(self,valid_data,is_train=True):\n",
    "        patched_data=[]\n",
    "        batch_size=32\n",
    "        opinions=['neg','neu','pos']\n",
    "        n_batch=int((len(valid_data)-1)/batch_size+1)\n",
    "        for i in range(n_batch):\n",
    "            items=valid_data[i*batch_size:(i+1)*batch_size]\n",
    "            doc=patchMatrix(items.WordIds.tolist())\n",
    "            wpos=patchMatrix(items.POSID.tolist())\n",
    "            position=patchMatrix(items.Position.tolist())\n",
    "            distance=patchMatrix(items.Distances.tolist())\n",
    "            sentiments=patchMatrix(items.Sentiment.tolist())\n",
    "\n",
    "            left=patchMatrix([list(a)+list(b) for a,b in zip(items.LeftIds.tolist(),items.ViewIds.tolist())])\n",
    "            right=patchMatrix([list(a)+list(b) for a,b in zip(items.ViewIds.tolist(),items.RightIds.tolist())])\n",
    "            if is_train:\n",
    "                ys=items.Opinion.apply(lambda x:opinions.index(x)).tolist()\n",
    "            else:\n",
    "                ys=items.Opinion.apply(lambda x:-1).tolist()\n",
    "            views=patchMatrix(items.ViewIds.tolist())\n",
    "            patched_data.append((left,right,views,items.index.tolist(),items.SentenceId.tolist(),ys))\n",
    "        return patched_data\n",
    "    \n",
    "\n",
    "    def get_xs(self,train_df,is_train=True):\n",
    "        train_data=self.get_patched_data(train_df,is_train)\n",
    "        train_xs=[list(item)[:3] for item in train_data]\n",
    "        train_ys=[self.lb.transform(item[-1]) for item in train_data]\n",
    "        return train_xs,train_ys\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data/car_review_data.pkl\n",
      "retrain and save model to best.tc_lstm.model\n",
      "output result to answer.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'POS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-31d245383964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#----------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;34m'''读取训练数据'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPOS_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuml/others/E/Projects2016/Python/DeepLearningPractice2017/SentimentAnalyze/yuml/datasets/gridsum2016.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filename, pos_rate, neg_rate)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mPOS_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mPOS_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mPOS_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPOS_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mPOS_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPOS_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPOS_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuml/lib/anaconda3/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2670\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'POS'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TC_LSTM神经网络模型\n",
    "输入process_data处理好的pkl文件，包括(data,vocs,id2words,w2v)\n",
    "输出情感分析结果\n",
    "by: liyumeng\n",
    "@ 2016/11/28\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from yuml.datasets.gridsum2016 import load_data,patchMatrix,get_patched_data\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    input_filename='data/car_review_data.pkl'\n",
    "    model_filename='best.tc_lstm.model'\n",
    "    output_filename='answer.csv'\n",
    "    retrain='1'\n",
    "    \n",
    "    print('input',input_filename)\n",
    "    if retrain=='1':\n",
    "        print('retrain and save model to',model_filename)\n",
    "    else:\n",
    "        print('load model from ',model_filename)\n",
    "    print('output result to',output_filename)\n",
    "    \n",
    "    #----------------------------------------------------------\n",
    "    '''读取训练数据'''\n",
    "    train_df,valid_df,test_df,vocs,w2v,POS_dict=load_data(input_filename,pos_rate=0.2,neg_rate=0.2)\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    '''开始训练'''\n",
    "\n",
    "    model=TC_LSTM(w2v)\n",
    "    train_data=model.get_xs(train_df)\n",
    "    valid_data=model.get_xs(valid_df)\n",
    "    test_data=model.get_xs(test_df,False)\n",
    "    if retrain =='1':\n",
    "        model.fit(train_data=train_data,valid_data=valid_data,filename=model_filename,best_type='best_acc')\n",
    "    else:\n",
    "        model.model.load_weights(model_filename)\n",
    "\n",
    "    #-------------------------------------------------------------------\n",
    "    '''预测并输出\n",
    "    '''\n",
    "    res=model.predict(test_data[0],test_data[1])\n",
    "\n",
    "    opinions=['neg','neu','pos']\n",
    "    yp=[opinions[np.argmax(r[0])] for r in res]\n",
    "    \n",
    "    print('输出的各类别比例：')\n",
    "    test_num=Counter(yp)\n",
    "    for key in test_num:\n",
    "        print(key,test_num[key]/len(test_df),test_num[key])\n",
    "    \n",
    "    test_df.loc[:,'Opinion']=yp\n",
    "    test_df.loc[:,['SentenceId','RawView','Opinion']].to_csv(output_filename,index=False,sep=',',encoding='utf8',header=True)\n",
    "    print('运行完毕！已输出到',output_filename)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6137cde4893c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
